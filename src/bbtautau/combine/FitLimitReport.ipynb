{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "from pathlib import Path\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Set up plotting with CMS style\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"font.size\"] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CARDS_BASE_DIR = \"/home/users/lumori/bbtautau/src/bbtautau/cards/26Jan6-ggf\"\n",
    "CHANNELS = [\"combined\", \"hh\", \"he\", \"hm\"]\n",
    "\n",
    "\n",
    "# Find all card directories with outputs\n",
    "def find_output_directories():\n",
    "    \"\"\"Find all directories containing analysis outputs\"\"\"\n",
    "    output_dirs = []\n",
    "\n",
    "    for root, dirs, files in os.walk(CARDS_BASE_DIR):\n",
    "        if \"outs\" in dirs:\n",
    "            outs_path = Path(root) / \"outs\"\n",
    "            if any(outs_path.glob(\"*.txt\")):\n",
    "                output_dirs.append(root)\n",
    "\n",
    "    return sorted(output_dirs)\n",
    "\n",
    "\n",
    "output_dirs = find_output_directories()\n",
    "print(f\"Found {len(output_dirs)} directories with outputs:\")\n",
    "for d in output_dirs:\n",
    "    print(f\"  - {Path(d).relative_to(CARDS_BASE_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_asymptotic_limits(file_path):\n",
    "    \"\"\"Parse asymptotic limits from log file\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    if not Path(file_path).exists():\n",
    "        return results\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Look for expected limits pattern\n",
    "        patterns = {\n",
    "            \"expected_2.5\": r\"Expected\\s+2\\.5%:\\s*r\\s*<\\s*([0-9.]+)\",\n",
    "            \"expected_16.0\": r\"Expected\\s+16\\.0%:\\s*r\\s*<\\s*([0-9.]+)\",\n",
    "            \"expected_50.0\": r\"Expected\\s+50\\.0%:\\s*r\\s*<\\s*([0-9.]+)\",\n",
    "            \"expected_84.0\": r\"Expected\\s+84\\.0%:\\s*r\\s*<\\s*([0-9.]+)\",\n",
    "            \"expected_97.5\": r\"Expected\\s+97\\.5%:\\s*r\\s*<\\s*([0-9.]+)\",\n",
    "            \"observed\": r\"Observed\\s+Limit:\\s*r\\s*<\\s*([0-9.]+)\",\n",
    "        }\n",
    "\n",
    "        for key, pattern in patterns.items():\n",
    "            match = re.search(pattern, content, re.IGNORECASE)\n",
    "            if match:\n",
    "                results[key] = float(match.group(1))\n",
    "\n",
    "        # Check for convergence issues\n",
    "        convergence_issues = []\n",
    "        if \"Minimization did NOT converge\" in content:\n",
    "            convergence_issues.append(\"did_not_converge\")\n",
    "\n",
    "        results[\"convergence_issues\"] = convergence_issues\n",
    "        results[\"status\"] = \"success\" if not convergence_issues else \"issues\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {file_path}: {e}\")\n",
    "        results[\"status\"] = \"parse_error\"\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_background_fit(file_path):\n",
    "    \"\"\"Parse background fit results\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    if not Path(file_path).exists():\n",
    "        return results\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Check fit status\n",
    "        if \"Minimization success!\" in content:\n",
    "            results[\"fit_converged\"] = True\n",
    "        elif \"Minimization did NOT converge\" in content:\n",
    "            results[\"fit_converged\"] = False\n",
    "        else:\n",
    "            results[\"fit_converged\"] = None\n",
    "\n",
    "        # Extract fit statistics\n",
    "        nll_match = re.search(r\"best fit NLL\\s*=\\s*([0-9.-]+)\", content)\n",
    "        if nll_match:\n",
    "            results[\"best_fit_nll\"] = float(nll_match.group(1))\n",
    "\n",
    "        # Count function calls\n",
    "        fcn_calls = len(re.findall(r\"FCN\", content))\n",
    "        results[\"function_calls\"] = fcn_calls\n",
    "\n",
    "        # Check for specific issues\n",
    "        issues = []\n",
    "        if \"Hesse matrix not pos-def\" in content:\n",
    "            issues.append(\"hesse_not_posdef\")\n",
    "        if \"MIGRAD FAILS\" in content:\n",
    "            issues.append(\"migrad_failed\")\n",
    "        if \"Covariance matrix\" in content and \"not available\" in content:\n",
    "            issues.append(\"no_covariance\")\n",
    "\n",
    "        results[\"fit_issues\"] = issues\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing background fit {file_path}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test parsing on first directory\n",
    "if output_dirs:\n",
    "    test_dir = Path(output_dirs[0]) / \"outs\"\n",
    "    print(f\"Testing parsing on: {test_dir}\")\n",
    "\n",
    "    # Look for limit files\n",
    "    limit_files = list(test_dir.glob(\"*AsymptoticLimits.txt\"))\n",
    "    if limit_files:\n",
    "        test_results = parse_asymptotic_limits(limit_files[0])\n",
    "        print(f\"Sample limit parsing: {test_results}\")\n",
    "\n",
    "    # Look for background fit files\n",
    "    bfit_files = list(test_dir.glob(\"*MultiDimFit.txt\"))\n",
    "    if bfit_files:\n",
    "        test_bfit = parse_background_fit(bfit_files[0])\n",
    "        print(f\"Sample background fit parsing: {test_bfit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal region labels: ggf, vbf, or allsigs (ggf+vbf combined)\n",
    "SIG_REGION_LABELS = [\"\", \"ggf\", \"vbf\", \"allsigs\"]\n",
    "\n",
    "\n",
    "def collect_all_results():\n",
    "    \"\"\"Collect results from all output directories\"\"\"\n",
    "    all_results = []\n",
    "\n",
    "    for card_dir in output_dirs:\n",
    "        outs_dir = Path(card_dir) / \"outs\"\n",
    "        card_name = Path(card_dir).relative_to(CARDS_BASE_DIR)\n",
    "\n",
    "        print(f\"Processing: {card_name}\")\n",
    "\n",
    "        # Check for different channel and signal region results\n",
    "        for channel in CHANNELS:\n",
    "            for siglabel in SIG_REGION_LABELS:\n",
    "                # Build label: siglabel + channel\n",
    "                channellabel = \"\" if channel == \"combined\" else channel\n",
    "                label = f\"{siglabel}{channellabel}\"\n",
    "\n",
    "                result_entry = {\n",
    "                    \"card_directory\": str(card_name),\n",
    "                    \"channel\": channel,\n",
    "                    \"sig_region\": siglabel,\n",
    "                }\n",
    "\n",
    "                # Look for limits file\n",
    "                limit_file = outs_dir / f\"{label}AsymptoticLimits.txt\"\n",
    "                bfit_file = outs_dir / f\"{label}MultiDimFit.txt\"\n",
    "\n",
    "                # Parse limits\n",
    "                limit_results = parse_asymptotic_limits(limit_file)\n",
    "                result_entry.update(limit_results)\n",
    "\n",
    "                # Parse background fit\n",
    "                bfit_results = parse_background_fit(bfit_file)\n",
    "                result_entry.update(bfit_results)\n",
    "\n",
    "                # Only add if we found some results\n",
    "                if limit_results or bfit_results:\n",
    "                    all_results.append(result_entry)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "\n",
    "# Collect all results\n",
    "results_df = collect_all_results()\n",
    "print(f\"\\nCollected results from {len(results_df)} analyses\")\n",
    "print(f\"Columns: {list(results_df.columns)}\")\n",
    "print(f\"\\nFirst few entries:\")\n",
    "display(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=== ANALYSIS SUMMARY ===\")\n",
    "print(f\"Total analyses: {len(results_df)}\")\n",
    "print(f\"Card directories: {results_df['card_directory'].nunique()}\")\n",
    "print(f\"Signal regions: {sorted([sr for sr in results_df['sig_region'].unique() if sr])}\")\n",
    "print(f\"Channels analyzed: {sorted(results_df['channel'].unique())}\")\n",
    "\n",
    "# Status summary\n",
    "if \"status\" in results_df.columns:\n",
    "    print(\"\\n=== LIMIT CALCULATION STATUS ===\")\n",
    "    status_counts = results_df[\"status\"].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"{status}: {count}\")\n",
    "\n",
    "# Convergence summary\n",
    "if \"fit_converged\" in results_df.columns:\n",
    "    print(\"\\n=== BACKGROUND FIT CONVERGENCE ===\")\n",
    "    conv_counts = results_df[\"fit_converged\"].value_counts()\n",
    "    for conv, count in conv_counts.items():\n",
    "        print(f\"{conv}: {count}\")\n",
    "\n",
    "# Expected limits summary by signal region and channel\n",
    "if \"expected_50.0\" in results_df.columns:\n",
    "    # Clean up names\n",
    "    channel_name_map = {\"combined\": \"Combined\", \"hh\": \"œÑ_h œÑ_h\", \"he\": \"œÑ_h e\", \"hm\": \"œÑ_h Œº\"}\n",
    "    sig_region_name_map = {\"ggf\": \"ggF\", \"vbf\": \"VBF\", \"all\": \"ggF+VBF\", \"allsigs\": \"ggF+VBF\"}\n",
    "\n",
    "    # Group by signal region and channel\n",
    "    limits_summary = results_df.groupby([\"sig_region\", \"channel\"])[\"expected_50.0\"].agg(\n",
    "        [\"count\", \"mean\", \"std\", \"min\", \"max\"]\n",
    "    )\n",
    "    limits_summary.columns = [\"Count\", \"Mean\", \"Std Dev\", \"Min\", \"Max\"]\n",
    "\n",
    "    # Rename index levels\n",
    "    limits_summary.index = limits_summary.index.set_levels(\n",
    "        [sig_region_name_map.get(sr, sr) for sr in limits_summary.index.levels[0]], level=0\n",
    "    )\n",
    "    limits_summary.index = limits_summary.index.set_levels(\n",
    "        [channel_name_map.get(ch, ch) for ch in limits_summary.index.levels[1]], level=1\n",
    "    )\n",
    "    limits_summary.index.names = [\"Signal Region\", \"Channel\"]\n",
    "\n",
    "    print(\"\\n=== EXPECTED LIMITS (50%) BY SIGNAL REGION AND CHANNEL ===\")\n",
    "    display(limits_summary.round(2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Detailed Results Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate transposed tables for each signal region and channel\n",
    "print(\"=== DETAILED RESULTS BY SIGNAL REGION AND CHANNEL ===\")\n",
    "\n",
    "# Get available columns\n",
    "limit_cols = [\"expected_2.5\", \"expected_16.0\", \"expected_50.0\", \"expected_84.0\", \"expected_97.5\"]\n",
    "available_limit_cols = [col for col in limit_cols if col in results_df.columns]\n",
    "\n",
    "status_cols = [\"status\", \"fit_converged\"]\n",
    "available_status_cols = [col for col in status_cols if col in results_df.columns]\n",
    "\n",
    "# Create table for each signal region and channel\n",
    "signal_regions = sorted([sr for sr in results_df[\"sig_region\"].unique() if sr])  # exclude empty\n",
    "channels = sorted(results_df[\"channel\"].unique())\n",
    "\n",
    "# Define nice column names\n",
    "column_name_map = {\n",
    "    \"expected_2.5\": \"Expected -2œÉ\",\n",
    "    \"expected_16.0\": \"Expected -1œÉ\",\n",
    "    \"expected_50.0\": \"Expected Median\",\n",
    "    \"expected_84.0\": \"Expected +1œÉ\",\n",
    "    \"expected_97.5\": \"Expected +2œÉ\",\n",
    "    \"status\": \"Status\",\n",
    "    \"fit_converged\": \"Fit Converged\",\n",
    "}\n",
    "\n",
    "# Define nice channel names\n",
    "channel_name_map = {\"combined\": \"Combined\", \"hh\": \"œÑ_h œÑ_h\", \"he\": \"œÑ_h œÑ_e\", \"hm\": \"œÑ_h œÑ_Œº\"}\n",
    "\n",
    "# Define nice signal region names\n",
    "sig_region_name_map = {\"ggf\": \"ggF\", \"vbf\": \"VBF\", \"all\": \"ggF+VBF\", \"allsigs\": \"ggF+VBF\"}\n",
    "\n",
    "for signal_region in signal_regions:\n",
    "    sig_region_display = sig_region_name_map.get(signal_region, signal_region.upper())\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Signal Region: {sig_region_display}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for channel in channels:\n",
    "        # Filter by both signal region and channel\n",
    "        filtered_data = results_df[\n",
    "            (results_df[\"sig_region\"] == signal_region) & (results_df[\"channel\"] == channel)\n",
    "        ].copy()\n",
    "\n",
    "        if len(filtered_data) > 0:\n",
    "            # Select columns (excluding 'channel' and 'sig_region' since they're the same for all rows)\n",
    "            summary_cols = [\"card_directory\"] + available_limit_cols + available_status_cols\n",
    "            summary_table = filtered_data[summary_cols].copy()\n",
    "\n",
    "            # Format the table - round to 2 decimal places\n",
    "            for col in available_limit_cols:\n",
    "                if col in summary_table.columns:\n",
    "                    summary_table[col] = summary_table[col].round(2)\n",
    "\n",
    "            # Clean up card_directory names (remove underscores, make prettier)\n",
    "            summary_table[\"card_directory\"] = (\n",
    "                summary_table[\"card_directory\"].str.replace(\"_\", \" \").str.title()\n",
    "            )\n",
    "\n",
    "            # Transpose the table: set card_directory as index, then transpose\n",
    "            transposed_table = summary_table.set_index(\"card_directory\").T\n",
    "\n",
    "            # Rename the index (row names) to be more readable\n",
    "            transposed_table.index = [\n",
    "                column_name_map.get(idx, idx.replace(\"_\", \" \").title())\n",
    "                for idx in transposed_table.index\n",
    "            ]\n",
    "\n",
    "            channel_display_name = channel_name_map.get(channel, channel.upper())\n",
    "            print(f\"\\n--- {channel_display_name} Channel ---\")\n",
    "            display(transposed_table)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(hep.style.CMS)\n",
    "hep.style.use(\"CMS\")\n",
    "\n",
    "from boostedhh import hh_vars\n",
    "\n",
    "years = [\"2022\", \"2022EE\", \"2023\", \"2023BPix\"]\n",
    "\n",
    "\n",
    "if \"expected_50.0\" in results_df.columns and len(results_df) > 0:\n",
    "    required_cols = [\n",
    "        \"expected_2.5\",\n",
    "        \"expected_16.0\",\n",
    "        \"expected_50.0\",\n",
    "        \"expected_84.0\",\n",
    "        \"expected_97.5\",\n",
    "    ]\n",
    "    plot_df = results_df.copy()\n",
    "\n",
    "    # Keep only rows with full bands available\n",
    "    plot_df = plot_df.dropna(subset=[col for col in required_cols if col in plot_df.columns])\n",
    "\n",
    "    # Get unique bmin values\n",
    "    bmins = sorted(plot_df[\"card_directory\"].unique())\n",
    "\n",
    "    # Nice names for display\n",
    "    channel_name_map = {\n",
    "        \"combined\": \"Combined\",\n",
    "        \"hh\": \"$œÑ_hœÑ_h$\",\n",
    "        \"he\": \"$œÑ_hœÑ_e$\",\n",
    "        \"hm\": \"$œÑ_hœÑ_Œº$\",\n",
    "    }\n",
    "    sig_region_name_map = {\"ggf\": \"ggF\", \"vbf\": \"VBF\", \"all\": \"ggF+VBF\", \"allsigs\": \"ggF+VBF\"}\n",
    "\n",
    "    if len(plot_df) > 0 and len(bmins) > 0:\n",
    "        for bmin in bmins:\n",
    "            # Filter data for this bmin\n",
    "            bmin_df = plot_df[plot_df[\"card_directory\"] == bmin].copy()\n",
    "\n",
    "            if len(bmin_df) == 0:\n",
    "                continue\n",
    "\n",
    "            # Create region labels: \"sig_region / channel\"\n",
    "            bmin_df[\"region_label\"] = bmin_df.apply(\n",
    "                lambda row: f\"{sig_region_name_map.get(row['sig_region'], row['sig_region'])} / {channel_name_map.get(row['channel'], row['channel'])}\",\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            # Sort by sig_region then channel with custom ordering (combined, hh, hm, he)\n",
    "            channel_order = [\"combined\", \"hh\", \"hm\", \"he\"]\n",
    "            bmin_df[\"channel\"] = pd.Categorical(\n",
    "                bmin_df[\"channel\"], categories=channel_order, ordered=True\n",
    "            )\n",
    "            bmin_df = bmin_df.sort_values([\"sig_region\", \"channel\"])\n",
    "\n",
    "            regions = bmin_df[\"region_label\"].tolist()\n",
    "            n_regions = len(regions)\n",
    "\n",
    "            if n_regions == 0:\n",
    "                continue\n",
    "\n",
    "            # Create figure\n",
    "            fig_height = max(4, 0.6 * n_regions + 2)\n",
    "            fig, ax = plt.subplots(figsize=(10, fig_height), dpi=400)\n",
    "\n",
    "            y_pos = np.arange(n_regions)\n",
    "            bar_height = 1.0  # Adjacent bars (no white space)\n",
    "\n",
    "            # Extract values\n",
    "            med = bmin_df[\"expected_50.0\"].values\n",
    "            low1 = bmin_df[\"expected_16.0\"].values\n",
    "            high1 = bmin_df[\"expected_84.0\"].values\n",
    "            low2 = bmin_df[\"expected_2.5\"].values\n",
    "            high2 = bmin_df[\"expected_97.5\"].values\n",
    "\n",
    "            # Brazil plot colors\n",
    "            yellow = \"#FFCC00\"  # 95% band (2œÉ)\n",
    "            green = \"#00CC00\"  # 68% band (1œÉ)\n",
    "\n",
    "            # Plot 95% (2œÉ) band - yellow\n",
    "            ax.barh(\n",
    "                y_pos,\n",
    "                high2 - low2,\n",
    "                left=low2,\n",
    "                height=bar_height,\n",
    "                color=yellow,\n",
    "                edgecolor=\"none\",\n",
    "                label=\"Expected ¬±2œÉ\",\n",
    "                zorder=1,\n",
    "            )\n",
    "\n",
    "            # Plot 68% (1œÉ) band - green\n",
    "            ax.barh(\n",
    "                y_pos,\n",
    "                high1 - low1,\n",
    "                left=low1,\n",
    "                height=bar_height,\n",
    "                color=green,\n",
    "                edgecolor=\"none\",\n",
    "                label=\"Expected ¬±1œÉ\",\n",
    "                zorder=2,\n",
    "            )\n",
    "\n",
    "            # Plot median expected - black dashed line\n",
    "            for i, (y, m) in enumerate(zip(y_pos, med)):\n",
    "                ax.plot(\n",
    "                    [m, m],\n",
    "                    [y - bar_height / 2, y + bar_height / 2],\n",
    "                    color=\"black\",\n",
    "                    linestyle=\"--\",\n",
    "                    linewidth=2,\n",
    "                    zorder=3,\n",
    "                    label=\"Median expected\" if i == 0 else None,\n",
    "                )\n",
    "\n",
    "            # Reference line at r=1\n",
    "            ax.axvline(x=1, color=\"red\", linestyle=\"-\", linewidth=1.5, alpha=0.7, label=\"SM (r=1)\")\n",
    "\n",
    "            # Formatting - keep y labels but hide tick marks\n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(regions, fontsize=14)\n",
    "\n",
    "            # Add expected values below each label in smaller font\n",
    "            for i, (y, m) in enumerate(zip(y_pos, med)):\n",
    "                ax.annotate(\n",
    "                    f\"Exp. {m:.1f}\",\n",
    "                    xy=(0, y + 0.07),\n",
    "                    xytext=(-10, -10),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha=\"right\",\n",
    "                    va=\"top\",\n",
    "                    fontsize=11,\n",
    "                    xycoords=(\"axes fraction\", \"data\"),\n",
    "                )\n",
    "            ax.set_xlabel(\"95% CL limit on $\\sigma(pp\\\\rightarrow HH) / \\sigma_{SM}$\", fontsize=16)\n",
    "            ax.set_ylabel(\"\")\n",
    "            ax.tick_params(axis=\"y\", length=0)  # Hide y tick marks, keep labels\n",
    "            ax.tick_params(axis=\"x\", which=\"major\", labelsize=14)\n",
    "\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_xscale(\"log\")\n",
    "            ax.grid(True, axis=\"x\", ls=\":\", alpha=0.5, which=\"both\")\n",
    "\n",
    "            # Legend\n",
    "            ax.legend(loc=\"upper right\", fontsize=12)\n",
    "\n",
    "            # Title\n",
    "            # ax.set_title(f\"Expected Limits - {bmin}\", fontsize=14, fontweight=\"bold\")\n",
    "            print(bmin)\n",
    "\n",
    "            # CMS label\n",
    "            hep.cms.label(\n",
    "                ax=ax,\n",
    "                label=\"Work in Progress\",\n",
    "                data=True,\n",
    "                year=\"2022-23\",\n",
    "                com=\"13.6\",\n",
    "                fontsize=14,\n",
    "                lumi=f\"{np.sum([hh_vars.LUMI[year] for year in years]) / 1000:.1f}\",\n",
    "            )\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No data to plot\")\n",
    "else:\n",
    "    print(\"Insufficient data for Brazil plots\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Issues and Warnings Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify problematic analyses\n",
    "print(\"=== ISSUES AND WARNINGS REPORT ===\")\n",
    "\n",
    "# Convergence issues\n",
    "if \"fit_converged\" in results_df.columns:\n",
    "    non_converged = results_df[results_df[\"fit_converged\"] == False]\n",
    "    if len(non_converged) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  NON-CONVERGED BACKGROUND FITS ({len(non_converged)})\")\n",
    "        for _, row in non_converged.iterrows():\n",
    "            sig_region = row.get(\"sig_region\", \"\")\n",
    "            print(f\"  - {row['card_directory']} / {sig_region} / {row['channel']}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All background fits converged\")\n",
    "\n",
    "# Limit calculation issues\n",
    "if \"status\" in results_df.columns:\n",
    "    failed_limits = results_df[results_df[\"status\"] != \"success\"]\n",
    "    if len(failed_limits) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  LIMIT CALCULATION ISSUES ({len(failed_limits)})\")\n",
    "        for _, row in failed_limits.iterrows():\n",
    "            sig_region = row.get(\"sig_region\", \"\")\n",
    "            issues = \", \".join(row.get(\"convergence_issues\", []))\n",
    "            print(\n",
    "                f\"  - {row['card_directory']} / {sig_region} / {row['channel']}: {row['status']} ({issues})\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All limit calculations successful\")\n",
    "\n",
    "# Outlier limits (unusually high or low)\n",
    "if \"expected_50.0\" in results_df.columns:\n",
    "    valid_limits = results_df.dropna(subset=[\"expected_50.0\"])\n",
    "    if len(valid_limits) > 1:\n",
    "        Q1 = valid_limits[\"expected_50.0\"].quantile(0.25)\n",
    "        Q3 = valid_limits[\"expected_50.0\"].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outliers = valid_limits[\n",
    "            (valid_limits[\"expected_50.0\"] < lower_bound)\n",
    "            | (valid_limits[\"expected_50.0\"] > upper_bound)\n",
    "        ]\n",
    "\n",
    "        if len(outliers) > 0:\n",
    "            print(f\"\\nüìä OUTLIER LIMITS ({len(outliers)})\")\n",
    "            print(f\"   Normal range: {lower_bound:.3f} - {upper_bound:.3f}\")\n",
    "            for _, row in outliers.iterrows():\n",
    "                sig_region = row.get(\"sig_region\", \"\")\n",
    "                print(\n",
    "                    f\"  - {row['card_directory']} / {sig_region} / {row['channel']}: {row['expected_50.0']:.3f}\"\n",
    "                )\n",
    "        else:\n",
    "            print(\"\\n‚úÖ No outlier limits detected\")\n",
    "\n",
    "# Function call warnings\n",
    "if \"function_calls\" in results_df.columns:\n",
    "    high_calls = results_df[results_df[\"function_calls\"] > 10000]  # Arbitrary threshold\n",
    "    if len(high_calls) > 0:\n",
    "        print(f\"\\n‚è±Ô∏è  HIGH FUNCTION CALL COUNT ({len(high_calls)})\")\n",
    "        for _, row in high_calls.iterrows():\n",
    "            sig_region = row.get(\"sig_region\", \"\")\n",
    "            print(\n",
    "                f\"  - {row['card_directory']} / {sig_region} / {row['channel']}: {row['function_calls']} calls\"\n",
    "            )\n",
    "\n",
    "print(\"\\n=== END REPORT ===\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Export Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV for further analysis\n",
    "output_file = Path(CARDS_BASE_DIR) / \"analysis_results_summary.csv\"\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# Create a summary report\n",
    "report_file = Path(CARDS_BASE_DIR) / \"analysis_report.txt\"\n",
    "with open(report_file, \"w\") as f:\n",
    "    f.write(\"HiggsCombine Analysis Report\\n\")\n",
    "    f.write(\"=\" * 30 + \"\\n\\n\")\n",
    "\n",
    "    f.write(f\"Generated: {pd.Timestamp.now()}\\n\\n\")\n",
    "\n",
    "    f.write(\"SUMMARY:\\n\")\n",
    "    f.write(f\"- Total analyses: {len(results_df)}\\n\")\n",
    "    f.write(f\"- Card directories: {results_df['card_directory'].nunique()}\\n\")\n",
    "    f.write(f\"- Channels: {', '.join(sorted(results_df['channel'].unique()))}\\n\\n\")\n",
    "\n",
    "    if \"expected_50.0\" in results_df.columns:\n",
    "        combined_limits = results_df[results_df[\"channel\"] == \"combined\"][\"expected_50.0\"].dropna()\n",
    "        if len(combined_limits) > 0:\n",
    "            f.write(\"COMBINED CHANNEL EXPECTED LIMITS:\\n\")\n",
    "            f.write(f\"- Best limit: {combined_limits.min():.3f}\\n\")\n",
    "            f.write(f\"- Median limit: {combined_limits.median():.3f}\\n\")\n",
    "            f.write(f\"- Worst limit: {combined_limits.max():.3f}\\n\\n\")\n",
    "\n",
    "    if \"fit_converged\" in results_df.columns:\n",
    "        conv_rate = results_df[\"fit_converged\"].sum() / len(results_df) * 100\n",
    "        f.write(f\"CONVERGENCE RATE: {conv_rate:.1f}%\\n\\n\")\n",
    "\n",
    "    f.write(\"DETAILED RESULTS: See analysis_results_summary.csv\\n\")\n",
    "\n",
    "print(f\"Summary report saved to: {report_file}\")\n",
    "print(\"\\nüìÅ Output files created:\")\n",
    "print(f\"  - {output_file.name}\")\n",
    "print(f\"  - {report_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
